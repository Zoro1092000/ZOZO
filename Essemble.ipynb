{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "4KD-KUsKDiLE",
        "igYJ4y99D-bU",
        "z_dcpDhREijU",
        "fTEnWpOOEZW9",
        "5fvRugR_E1QL",
        "8rh6Ca1XE6ar",
        "HWrzKUrFFcNV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zoro1092000/ZOZO/blob/master/Essemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Pip & Import."
      ],
      "metadata": {
        "id": "4KD-KUsKDiLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7D6x31EU0fi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric==1.4.3\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install deepdish==0.3.5\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.utils import scatter_\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_scatter import scatter_add\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "from itertools import chain\n",
        "import pickle\n",
        "import h5py\n",
        "import deepdish as dd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "import time\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_since(start):\n",
        "    now = time.time()\n",
        "    s = now - start\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    h = math.floor(m / 60)\n",
        "    m -= h * 60\n",
        "    if h == 0:\n",
        "        if m == 0:\n",
        "            return '%ds' % s\n",
        "        else:\n",
        "            return '%dm %ds' % (m, s)\n",
        "    else:\n",
        "        return '%dh %dm %ds' % (h, m, s)\n"
      ],
      "metadata": {
        "id": "X5j9SrCo4uM5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Data"
      ],
      "metadata": {
        "id": "r0VJxZnIDpJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data utils\n",
        "def h5group_to_dict(h5group):\n",
        "    group_dict = {k: v[()] for k, v in chain(h5group.items(), h5group.attrs.items())}\n",
        "    return group_dict\n",
        "\n",
        "def sub_dict(full_dict, *keys, to_tensor):\n",
        "    return {k: torch.tensor(full_dict[k]) if to_tensor else full_dict[k] for k in keys if k in full_dict}\n",
        "\n",
        "def build_graph_from_dict_pyg(graph_dict, to_tensor=True):\n",
        "    from torch_geometric.data import Data\n",
        "\n",
        "    g = Data(**sub_dict(graph_dict, 'edge_index', 'x', 'y', 'edge_attr', 'edge_y', to_tensor=to_tensor))\n",
        "    return g\n",
        "\n",
        "# Data loader\n",
        "class GraphDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, batch_size=128, shuffle=False, num_workers=0):\n",
        "\n",
        "        def collate_graph(graph_obj_list):\n",
        "            from torch_geometric.data import Batch\n",
        "            batch = Batch.from_data_list(graph_obj_list)\n",
        "            return batch\n",
        "\n",
        "        super().__init__(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=collate_graph,\n",
        "            num_workers=num_workers)\n",
        "\n",
        "# BotnetDataset\n",
        "class BotnetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, name='chord', root='data/botnet', split='train', graph_format='pyg', split_idx=None, add_nfeat_ones=True,\n",
        "                 in_memory=True):\n",
        "        super().__init__()\n",
        "        assert name in ['chord', 'debru', 'kadem', 'leet', 'c2', 'p2p']\n",
        "        assert split in ['train', 'val', 'test']\n",
        "\n",
        "        if isinstance(root, str):\n",
        "            root = osp.expanduser(osp.normpath(root))\n",
        "\n",
        "        self.name = name\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.split_idx = split_idx\n",
        "        self.add_nfeat_ones = add_nfeat_ones\n",
        "\n",
        "        self._graph_format = graph_format\n",
        "        if split == 'train':\n",
        "            self.path = self.processed_paths[0]\n",
        "        elif split == 'val':\n",
        "            self.path = self.processed_paths[1]\n",
        "        elif split == 'test':\n",
        "            self.path = self.processed_paths[2]\n",
        "\n",
        "        # in_memory = False\n",
        "        # self.data = h5py.File(self.path, 'r')\n",
        "        self.data = None    # defer opening file in each process to make multiprocessing work\n",
        "        self.data_type = 'file'\n",
        "        with h5py.File(self.path, 'r') as f:\n",
        "            self.num_graphs = f.attrs['num_graphs']\n",
        "            \n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.root, 'processed')\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [self.name + '_' + s + '.hdf5' for s in ('train', 'val', 'test')]\n",
        "\n",
        "    @property\n",
        "    def processed_paths(self):\n",
        "        return [osp.join(self.processed_dir, f) for f in self.processed_file_names]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_graphs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.data_type == 'dict':\n",
        "            graph_dict = self.data[str(index)]\n",
        "        elif self.data_type == 'file':\n",
        "            if self.data is None:\n",
        "                # only open once in each process\n",
        "                self.data = h5py.File(self.path, 'r')\n",
        "            graph_dict = h5group_to_dict(self.data[str(index)])\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        # graph_format == 'pyg':\n",
        "        return build_graph_from_dict_pyg(graph_dict)\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self.num_graphs):\n",
        "            yield self[i]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(topology: {self.name} | split: {self.split} | ' \\\n",
        "               f'#graphs: {len(self)} | graph format: {self.graph_format})'\n"
      ],
      "metadata": {
        "id": "JHHcJ7B-4BmU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Measure Performancce"
      ],
      "metadata": {
        "id": "igYJ4y99D-bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Metrics"
      ],
      "metadata": {
        "id": "z_dcpDhREijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(target, pred, label):\n",
        "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    tp = np.sum((target==label) & (pred==label))\n",
        "    fp = np.sum((target!=label) & (pred==label))\n",
        "    fn = np.sum((pred!=label) & (target==label))\n",
        "    \n",
        "    if tp+fp==0 or tp+fn==0:\n",
        "      return np.nan\n",
        "\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    \n",
        "    if precision+recall==0:\n",
        "      return np.nan\n",
        "      \n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def f1_macro(pred, target):\n",
        "    return np.mean([f1(target, pred, label) for label in range(0, 2)])\n",
        "\n",
        "\n",
        "def accuracy(pred, target):\n",
        "    return (pred == target).sum().item() / len(target)\n",
        "\n",
        "\n",
        "def true_positive(pred, target):\n",
        "    return (target[pred == 1] == 1).sum().item()\n",
        "\n",
        "\n",
        "def false_positive(pred, target):\n",
        "    return (target[pred == 1] == 0).sum().item()\n",
        "\n",
        "\n",
        "def true_negative(pred, target):\n",
        "    return (target[pred == 0] == 0).sum().item()\n",
        "\n",
        "\n",
        "def false_negative(pred, target):\n",
        "    return (target[pred == 0] == 1).sum().item()\n",
        "\n",
        "\n",
        "def recall(pred, target):\n",
        "    try:\n",
        "        return true_positive(pred, target) / (target == 1).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def precision(pred, target):\n",
        "    try:\n",
        "        prec = true_positive(pred, target) / (pred == 1).sum().item()\n",
        "        return prec\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def f1_score(pred, target):\n",
        "    prec = precision(pred, target)\n",
        "    rec = recall(pred, target)\n",
        "    try:\n",
        "        return 2 * (prec * rec) / (prec + rec)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def false_positive_rate(pred, target):\n",
        "    try:\n",
        "        return false_positive(pred, target) / (target == 0).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def false_negative_rate(pred, target):\n",
        "    try:\n",
        "        return false_negative(pred, target) / (target == 1).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n"
      ],
      "metadata": {
        "id": "BN-QIjMREw-7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Evaluation"
      ],
      "metadata": {
        "id": "fTEnWpOOEZW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metrics(target, pred_prob, threshold=0.5):\n",
        "    if isinstance(target, torch.Tensor):\n",
        "        target = target.cpu().numpy()\n",
        "    if isinstance(pred_prob, torch.Tensor):\n",
        "        pred_prob = pred_prob.cpu().numpy()\n",
        "\n",
        "    pred = (pred_prob >= threshold).astype(int)\n",
        "\n",
        "    acc = accuracy(pred, target)\n",
        "    fpr = false_positive_rate(pred, target)\n",
        "    fnr = false_negative_rate(pred, target)\n",
        "    rec = recall(pred, target)\n",
        "    prc = precision(pred, target)\n",
        "    f1 = f1_score(pred, target)\n",
        "    f1macro = f1_macro(pred, target)\n",
        "    result_dict = {'acc': acc, 'fpr': fpr, 'fnr': fnr, 'rec': rec, 'prc': prc, 'f1': f1, 'f1_macro': f1macro}\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "\n",
        "def dict_value_add(dict1, dict2):\n",
        "    result = {key: dict1.get(key, 0) + dict2.get(key, 0)\n",
        "              for key in set(dict1) | set(dict2)}\n",
        "    return result\n",
        "\n",
        "\n",
        "def dict_value_div(dict, n):\n",
        "    result = {key: value / n for key, value in dict.items()}\n",
        "    return result\n",
        "\n",
        "\n",
        "def eval_predictor(dataset, predictor):\n",
        "    result_dict_avg = {}\n",
        "    loss_avg = 0\n",
        "\n",
        "    for data in dataset:\n",
        "        # prediction\n",
        "        try:\n",
        "            pred_prob, loss = predictor(data)\n",
        "            loss_avg += loss\n",
        "        except ValueError:  # if \"too many values to unpack\"\n",
        "            pred_prob = predictor(data)\n",
        "\n",
        "        # get the ground truth target\n",
        "        # graph_format == 'pyg':\n",
        "        target = data.y\n",
        "\n",
        "        # compute the evaluation metrics\n",
        "        result_dict = eval_metrics(target, pred_prob)\n",
        "\n",
        "        result_dict_avg = dict_value_add(result_dict_avg, result_dict)\n",
        "\n",
        "    # average the metrics across all graphs in the dataset as final results\n",
        "    result_dict_avg = dict_value_div(result_dict_avg, len(dataset))\n",
        "    loss_avg = loss_avg / len(dataset)\n",
        "\n",
        "    return result_dict_avg, loss_avg\n",
        "\n",
        "\n",
        "# =================================================================================================================\n",
        "# some examples of the 'predictor' model wrapper to be fed into the above evaluation function (for PyG Data format)\n",
        "# =================================================================================================================\n",
        "class PygRandomPredictor:\n",
        "    def __init__(self):\n",
        "        # torch.manual_seed(0)\n",
        "        pass\n",
        "\n",
        "    def __call__(self, data):\n",
        "        pred_prob = torch.rand(len(data.y))\n",
        "        return pred_prob\n",
        "\n",
        "\n",
        "class PygModelPredictor:\n",
        "    def __init__(self, model, loss_fcn=torch.nn.CrossEntropyLoss()):\n",
        "        self.model = model\n",
        "        self.loss_fcn = loss_fcn\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def __call__(self, data):\n",
        "        self.model.eval()\n",
        "        data = data.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            # custom the below line to adjust to your model's input format for forward pass\n",
        "            out = self.model(data.x, data.edge_index)\n",
        "            loss = self.loss_fcn(out, data.y.long())\n",
        "            pred_prob = torch.softmax(out, dim=1)[:, 1]\n",
        "        return pred_prob, loss.float()\n"
      ],
      "metadata": {
        "id": "pqjalkx1EmHc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Model"
      ],
      "metadata": {
        "id": "5fvRugR_E1QL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. Activation "
      ],
      "metadata": {
        "id": "8rh6Ca1XE6ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(act, negative_slope=0.2):\n",
        "    activations = nn.ModuleDict([\n",
        "        ['lrelu', nn.LeakyReLU(negative_slope)],\n",
        "        ['relu', nn.ReLU()],\n",
        "        ['elu', nn.ELU()],\n",
        "        ['none', nn.Identity()],\n",
        "    ])\n",
        "    return activations[act]"
      ],
      "metadata": {
        "id": "bzEVz1VwFjKK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2. GIN model"
      ],
      "metadata": {
        "id": "HWrzKUrFFcNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GINModel(nn.Module):\n",
        "    def __init__(self, dim_input_feature, dim_hidden_feature, num_layers, num_classes, non_linear='relu', dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_input_feature = dim_input_feature\n",
        "        self.dim_hidden_feature = dim_hidden_feature\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
        "        from torch_geometric.nn import GCNConv, GINConv\n",
        "        \n",
        "        self.gin_net = nn.ModuleList()\n",
        "        for i in range(self.num_layers):\n",
        "          if i == 0:\n",
        "            self.gin_net.append(\n",
        "                GINConv(\n",
        "                  Sequential(Linear(self.dim_input_feature, self.dim_hidden_feature),\n",
        "                             BatchNorm1d(self.dim_hidden_feature), ReLU(),\n",
        "                             Linear(self.dim_hidden_feature, self.dim_hidden_feature), ReLU()), train_eps = True))\n",
        "          else:\n",
        "            self.gin_net.append(\n",
        "                GINConv(\n",
        "                  Sequential(Linear(self.dim_hidden_feature, self.dim_hidden_feature),\n",
        "                             BatchNorm1d(self.dim_hidden_feature), ReLU(),\n",
        "                             Linear(self.dim_hidden_feature, self.dim_hidden_feature), ReLU()), train_eps = True)) \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.residuals = nn.ModuleList()\n",
        "        for i in range(self.num_layers):\n",
        "          if i == 0:\n",
        "            self.residuals.append(nn.Linear(self.dim_input_feature, self.dim_hidden_feature, bias=True))\n",
        "          else:\n",
        "            self.residuals.append(nn.Identity())\n",
        "\n",
        "        self.num_residuals = len(self.residuals)\n",
        "\n",
        "        self.non_linear = activation(non_linear)\n",
        "\n",
        "        # self.final_type == 'proj':\n",
        "        self.final = nn.Linear(self.dim_hidden_feature, num_classes)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for net in self.gin_net:\n",
        "            net.reset_parameters()\n",
        "        # self.residual_hop = 1\n",
        "        for net in self.residuals:\n",
        "            net.reset_parameters()\n",
        "        # self.final_type != 'none':\n",
        "        self.final.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        xr = None\n",
        "        add_xr_at = -1\n",
        "\n",
        "        for n, net in enumerate(self.gin_net):\n",
        "            # pass to a GIN layer with non-linear activation\n",
        "            xo = net(x, edge_index)\n",
        "            xo = self.dropout(xo)\n",
        "            # deal with residual connections\n",
        "            # self.residual_hop = 1\n",
        "            if n < self.num_residuals:\n",
        "                xr = self.residuals[n](x)\n",
        "                add_xr_at = n\n",
        "            if n == add_xr_at:\n",
        "                xo = self.non_linear(xo + xr)\n",
        "\n",
        "            x = xo\n",
        "        # size of x: (B * N, dim_hidden_feature) -> (B * N, num_classes)\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "vLv8xNFcFqcT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Load data"
      ],
      "metadata": {
        "id": "RRCYp7yuLxMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/Shareddrives/botnetdata/P2P'\n",
        "data_name = 'p2p' # 'chord', 'debru', 'kadem', 'leet', 'c2', 'p2p'\n",
        "batch_size = 8\n",
        "in_memory = False\n",
        "shuffle = False\n",
        "\n",
        "# ========== load the dataset\n",
        "print('loading dataset...')\n",
        "\n",
        "train_ds = BotnetDataset(name=data_name, root=data_dir, split='train',\n",
        "                         in_memory=bool(in_memory), graph_format='pyg')\n",
        "val_ds = BotnetDataset(name=data_name, root=data_dir, split='val',\n",
        "                       in_memory=bool(in_memory), graph_format='pyg')\n",
        "test_ds = BotnetDataset(name=data_name, root=data_dir, split='test',\n",
        "                        in_memory=bool(in_memory), graph_format='pyg')\n",
        "train_loader = GraphDataLoader(train_ds, batch_size=batch_size, shuffle=bool(shuffle), num_workers=0)\n",
        "print(data_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msBAy6CKLwl8",
        "outputId": "5e7a0597-3b03-4a68-93c1-20fec08395a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dataset...\n",
            "p2p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. Train"
      ],
      "metadata": {
        "id": "PSY11P9WEX5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============== some default parameters =============\n",
        "# devid = 0\n",
        "# seed = 0\n",
        "# logmode = 'w'\n",
        "# log_interval = 96\n",
        "\n",
        "# dim_input_feature = 1\n",
        "# dim_hidden_feature = 32\n",
        "# act = 'relu' # 'none', 'lrelu', 'relu', 'elu'\n",
        "\n",
        "# num_layers = 12\n",
        "# num_classes = 2 \n",
        "\n",
        "# deg_norm = 'sm'\n",
        "# aggr = 'add' # 'add', 'mean', 'max'\n",
        "# dropout = 0.0\n",
        "# bias = True\n",
        "\n",
        "# lr = 0.005 # learning rate\n",
        "# weight_decay = 5e-4\n",
        "# epochs = 40\n",
        "# save_dir = './saved_models'\n",
        "# save_name = 'temp.pt'\n",
        "# # ====================================================\n",
        "\n",
        "# def train(model, train_loader, val_dataset, test_dataset, optimizer, scheduler=None):\n",
        "#     device = next(model.parameters()).device\n",
        "#     predictor = PygModelPredictor(model)\n",
        "\n",
        "#     best_epoch = 0\n",
        "#     min_avg_lost = np.inf\n",
        "#     start = time.time()\n",
        "#     for ep in range(epochs):\n",
        "#         loss_avg_train = 0\n",
        "#         num_train_graph = 0\n",
        "#         model.train()\n",
        "#         for n, batch in enumerate(train_loader):\n",
        "#             batch.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             x = model(batch.x, batch.edge_index)\n",
        "#             loss = criterion(x, batch.y.long())\n",
        "\n",
        "#             loss_avg_train += float(loss)\n",
        "#             num_train_graph += batch.num_graphs\n",
        "\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             if num_train_graph % log_interval == 0 or n == len(train_loader) - 1:\n",
        "#                 with torch.no_grad():\n",
        "#                     # pred = x.argmax(dim=1)\n",
        "#                     pred_prob = torch.softmax(x, dim=1)[:, 1]\n",
        "#                     y = batch.y.long()\n",
        "#                     result_dict = eval_metrics(y, pred_prob)\n",
        "#                 print(f'epoch: {ep + 1}, passed number of graphs: {num_train_graph}, '\n",
        "#                         f'train running loss: {loss_avg_train / num_train_graph:.5f} (passed time: {time_since(start)})')\n",
        "#                 print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict.items()]))\n",
        "\n",
        "#         result_dict_avg, loss_avg = eval_predictor(val_dataset, predictor)\n",
        "#         print(f'Validation --- epoch: {ep + 1}, loss: {loss_avg:.5f}')\n",
        "#         print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))\n",
        "\n",
        "#         if scheduler is not None:\n",
        "#             scheduler.step(loss_avg)\n",
        "\n",
        "#         if loss_avg < min_avg_lost:\n",
        "#             torch.save(model, os.path.join(save_dir, save_name))\n",
        "#             print(f'Better model saved at {os.path.join(save_dir, save_name)}.')\n",
        "#             best_epoch = ep\n",
        "#             min_avg_lost = loss_avg\n",
        "#             # Tai model ve de essemble learning\n",
        "\n",
        "#     best_model = torch.load(os.path.join(save_dir, save_name))\n",
        "#     print('*' * 12 + f' best model obtained after epoch {best_epoch + 1}, '\n",
        "#                        f'saved at {os.path.join(save_dir, save_name)} ' + '*' * 12)\n",
        "    \n",
        "#     predictor = PygModelPredictor(best_model)\n",
        "\n",
        "#     result_dict_avg, loss_avg = eval_predictor(test_dataset, predictor)\n",
        "#     print(f'Testing --- loss: {loss_avg:.5f}')\n",
        "#     print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     # ========== random seeds and device\n",
        "#     random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "\n",
        "#     device = torch.device(f'cuda:{devid}') if devid > -1 else torch.device('cpu')\n",
        "\n",
        "#     # ========== logging setup\n",
        "#     log_name = os.path.splitext(save_name)[0]\n",
        "#     # logger = logging_config(__name__, folder=save_dir, name=log_name, filemode=logmode)\n",
        "#     # logger = logging_config(os.path.basename(__file__), folder=save_dir, name=log_name, filemode=logmode)\n",
        "\n",
        "#     print('python ' + ' '.join(sys.argv))\n",
        "#     print('-' * 30)\n",
        "#     #logger.info(args)\n",
        "#     print('-' * 30)\n",
        "#     print(time.ctime())\n",
        "#     print('-' * 30)\n",
        "\n",
        "#     # ========== define the model, optimizer, and loss\n",
        "\n",
        "#     model = GINModel(dim_input_feature,\n",
        "#                      dim_hidden_feature,\n",
        "#                      num_layers,\n",
        "#                      num_classes,\n",
        "#                      non_linear=act,\n",
        "#                      dropout=dropout)\n",
        "\n",
        "#     print('model ' + '-' * 10)\n",
        "#     print(repr(model))\n",
        "#     model.to(device)\n",
        "\n",
        "#     class_weight = torch.Tensor([0.02, 0.98])\n",
        "#     class_weight = class_weight.cuda(device=0)\n",
        "#     criterion = torch.nn.CrossEntropyLoss(weight=class_weight)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=1)\n",
        "\n",
        "#     # ========== train the model\n",
        "#     train(model, train_loader, val_ds, test_ds, optimizer, scheduler)\n"
      ],
      "metadata": {
        "id": "3iFDupzCdBl0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII. Essemble Learning."
      ],
      "metadata": {
        "id": "qPAucvcZywEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GIN = torch.load(\"/content/saved_models/GIN_model.pt\")\n",
        "GCN = torch.load(\"/content/saved_models/GCN_model.pt\")"
      ],
      "metadata": {
        "id": "6GgRMsPNy7_y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PygModelPredictorEssemble:\n",
        "    def __init__(self, modelGIN, modelGCN, loss_fcn=torch.nn.CrossEntropyLoss()):\n",
        "        self.modelGIN = modelGIN\n",
        "        self.modelGCN = modelGCN\n",
        "        self.loss_fcn = loss_fcn\n",
        "        self.device = next(modelGIN.parameters()).device\n",
        "        self.device = next(modelGCN.parameters()).device\n",
        "\n",
        "    def __call__(self, data):\n",
        "        self.modelGIN.eval()\n",
        "        self.modelGCN.eval()\n",
        "        data = data.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            # custom the below line to adjust to your model's input format for forward pass\n",
        "            outGIN = self.modelGIN(data.x, data.edge_index)\n",
        "            outGCN = self.modelGCN(data.x, data.edge_index)\n",
        "            loss = self.loss_fcn((outGIN+2*outGCN)/3, data.y.long())\n",
        "            pred_prob = torch.softmax((outGIN+2*outGCN)/3, dim=1)[:, 1]\n",
        "        return pred_prob, loss.float()"
      ],
      "metadata": {
        "id": "polYZhDS4-Na"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictorGIN = PygModelPredictor(GIN)\n",
        "predictorGCN = PygModelPredictor(GCN)"
      ],
      "metadata": {
        "id": "Q-Uck7OB2d72"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict_avg, loss_avg = eval_predictor(test_ds, predictorGIN)\n",
        "print(f'Testing --- loss: {loss_avg:.5f}')\n",
        "print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgXntqKD2n4z",
        "outputId": "c9bbf581-af02-40ae-f34a-86439e93a490"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing --- loss: 0.01711\n",
            "          acc: 0.99707, prc: 0.96452, fpr: 0.00091, f1_macro: 0.96026, rec: 0.90574, fnr: 0.09426, f1: 0.92201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict_avg, loss_avg = eval_predictor(test_ds, predictorGCN)\n",
        "print(f'Testing --- loss: {loss_avg:.5f}')\n",
        "print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y4wdQQ927H6",
        "outputId": "a123f9ae-bf70-4159-c156-8da3c8e67ef6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing --- loss: 0.00781\n",
            "          acc: 0.99886, prc: 0.95912, fpr: 0.00093, f1_macro: 0.98658, rec: 0.98927, fnr: 0.01073, f1: 0.97375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictorEssemble = PygModelPredictorEssemble(GIN, GCN)"
      ],
      "metadata": {
        "id": "4-HIkSW0369x"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict_avg, loss_avg = eval_predictor(test_ds, predictorEssemble)\n",
        "print(f'Testing --- loss: {loss_avg:.5f}')\n",
        "print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXh0_5db3Z2r",
        "outputId": "6670353f-d98a-4de3-e841-7bbf2cf3b836"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing --- loss: 0.00657\n",
            "          acc: 0.99967, prc: 0.99791, fpr: 0.00004, f1_macro: 0.99586, rec: 0.98655, fnr: 0.01345, f1: 0.99190\n"
          ]
        }
      ]
    }
  ]
}