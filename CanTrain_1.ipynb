{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "r0VJxZnIDpJT",
        "mC6ZYfk2DuH7",
        "UDLJ9FOGDzUb",
        "igYJ4y99D-bU",
        "_axvEgpiERb7",
        "8rh6Ca1XE6ar",
        "axjMlKrAFX-s",
        "HWrzKUrFFcNV",
        "RRCYp7yuLxMe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zoro1092000/ZOZO/blob/master/CanTrain_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Pip & Import."
      ],
      "metadata": {
        "id": "4KD-KUsKDiLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H7D6x31EU0fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0bedc4-39c7-4c3e-d256-fe34a312361f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-geometric==1.4.3 in /usr/local/lib/python3.7/dist-packages (1.4.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (0.18.3)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (0.7.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (1.3.5)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (0.4)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (6.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (2.23.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (0.56.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (3.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (2.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric==1.4.3) (1.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->torch-geometric==1.4.3) (1.5.2)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric==1.4.3) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric==1.4.3) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric==1.4.3) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->torch-geometric==1.4.3) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->torch-geometric==1.4.3) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric==1.4.3) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric==1.4.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric==1.4.3) (1.15.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric==1.4.3) (3.0.9)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric==1.4.3) (0.6.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.4.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.4.3) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.4.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==1.4.3) (3.0.4)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->torch-geometric==1.4.3) (1.3.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->torch-geometric==1.4.3) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->torch-geometric==1.4.3) (2.9.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->torch-geometric==1.4.3) (7.1.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->torch-geometric==1.4.3) (2021.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric==1.4.3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric==1.4.3) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==1.4.3) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==1.4.3) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepdish==0.3.5 in /usr/local/lib/python3.7/dist-packages (0.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepdish==0.3.5) (1.7.3)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deepdish==0.3.5) (3.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepdish==0.3.5) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tables->deepdish==0.3.5) (21.3)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables->deepdish==0.3.5) (2.8.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tables->deepdish==0.3.5) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric==1.4.3\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install deepdish==0.3.5\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.utils import scatter_\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_scatter import scatter_add\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from itertools import chain\n",
        "import pickle\n",
        "import h5py\n",
        "import deepdish as dd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "import time\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Data"
      ],
      "metadata": {
        "id": "r0VJxZnIDpJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Data Utils"
      ],
      "metadata": {
        "id": "mC6ZYfk2DuH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def h5group_to_dict(h5group):\n",
        "    group_dict = {k: v[()] for k, v in chain(h5group.items(), h5group.attrs.items())}\n",
        "    return group_dict\n",
        "\n",
        "def sub_dict(full_dict, *keys, to_tensor):\n",
        "    return {k: torch.tensor(full_dict[k]) if to_tensor else full_dict[k] for k in keys if k in full_dict}\n",
        "\n",
        "def build_graph_from_dict_pyg(graph_dict, to_tensor=True):\n",
        "    from torch_geometric.data import Data\n",
        "\n",
        "    g = Data(**sub_dict(graph_dict, 'edge_index', 'x', 'y', 'edge_attr', 'edge_y', to_tensor=to_tensor))\n",
        "    return g\n",
        "\n"
      ],
      "metadata": {
        "id": "AtJueze12qRY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Data Loader"
      ],
      "metadata": {
        "id": "UDLJ9FOGDzUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, batch_size=1, shuffle=False, num_workers=0):\n",
        "\n",
        "        def collate_graph(graph_obj_list):\n",
        "            from torch_geometric.data import Batch\n",
        "            batch = Batch.from_data_list(graph_obj_list)\n",
        "            return batch\n",
        "\n",
        "        super().__init__(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=collate_graph,\n",
        "            num_workers=num_workers)\n"
      ],
      "metadata": {
        "id": "oBAPFSi3D3zp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Botnet Dataset"
      ],
      "metadata": {
        "id": "QPinpIJlD4Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def files_exist(files):\n",
        "    return all([osp.exists(f) for f in files])\n",
        "\n",
        "def extract_tar(path, folder, mode='r:gz', log=True):\n",
        "    print('Extracting', path)\n",
        "    import tarfile\n",
        "    with tarfile.open(path, mode) as f:\n",
        "        f.extractall(folder)\n",
        "\n",
        "def makedirs(path):\n",
        "    os.makedirs(osp.expanduser(osp.normpath(path)))\n",
        "# TODO: add degree calculation in preprocessing\n",
        "class BotnetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, name='chord', root='data/botnet', split='train', graph_format='pyg', split_idx=None, add_nfeat_ones=True,\n",
        "                 in_memory=True):\n",
        "        super().__init__()\n",
        "        assert name in ['chord', 'debru', 'kadem', 'leet', 'c2', 'p2p']\n",
        "        assert split in ['train', 'val', 'test']\n",
        "\n",
        "        if isinstance(root, str):\n",
        "            root = osp.expanduser(osp.normpath(root))\n",
        "\n",
        "        self.name = name\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.split_idx = split_idx\n",
        "        self.add_nfeat_ones = add_nfeat_ones\n",
        "\n",
        "        self.process()\n",
        "\n",
        "        self.in_memory = in_memory\n",
        "        self._graph_format = graph_format\n",
        "        if split == 'train':\n",
        "            self.path = self.processed_paths[0]\n",
        "        elif split == 'val':\n",
        "            self.path = self.processed_paths[1]\n",
        "        elif split == 'test':\n",
        "            self.path = self.processed_paths[2]\n",
        "\n",
        "        if in_memory:\n",
        "            self.data = dd.io.load(self.path)  # dictionary\n",
        "            self.data_type = 'dict'\n",
        "            self.num_graphs = self.data['num_graphs']\n",
        "        else:\n",
        "            # self.data = h5py.File(self.path, 'r')\n",
        "            self.data = None    # defer opening file in each process to make multiprocessing work\n",
        "            self.data_type = 'file'\n",
        "            with h5py.File(self.path, 'r') as f:\n",
        "                self.num_graphs = f.attrs['num_graphs']\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "        return osp.join(self.root, 'raw')\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.root, 'processed')\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['botnet_' + self.name + '.tar.gz', self.name + '_raw.hdf5', self.name + '_split_idx.pkl']\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [self.name + '_' + s + '.hdf5' for s in ('train', 'val', 'test')]\n",
        "\n",
        "    @property\n",
        "    def raw_paths(self):\n",
        "        return [osp.join(self.raw_dir, f) for f in self.raw_file_names]\n",
        "\n",
        "    @property\n",
        "    def processed_paths(self):\n",
        "        return [osp.join(self.processed_dir, f) for f in self.processed_file_names]\n",
        "\n",
        "    def process(self):\n",
        "        if files_exist(self.processed_paths):\n",
        "            return\n",
        "\n",
        "        if not files_exist(self.raw_paths[1:3]):\n",
        "            assert osp.exists(self.raw_paths[0])\n",
        "            path = extract_tar(self.raw_paths[0], self.raw_dir)\n",
        "\n",
        "        print('Processing...')\n",
        "        makedirs(self.processed_dir)\n",
        "\n",
        "        if self.split_idx is None:\n",
        "            # default data split\n",
        "            split_idx = pickle.load(open(self.raw_paths[2], 'rb'))\n",
        "\n",
        "        with h5py.File(self.raw_paths[1], 'r') as f:\n",
        "            for path, split in zip(self.processed_paths, ('train', 'val', 'test')):\n",
        "                print(f'writing {split} set ' + '-' * 10)\n",
        "                ori_graph_ids = split_idx[split]\n",
        "                with h5py.File(path, 'w') as g:\n",
        "                    num_nodes_sum = 0\n",
        "                    num_edges_sum = 0\n",
        "                    num_evils_sum = 0\n",
        "                    if 'num_evil_edges_avg' in f.attrs:\n",
        "                        num_evil_edges_sum = 0\n",
        "                        num_evil_edges_flag = True\n",
        "                    else:\n",
        "                        num_evil_edges_sum = None\n",
        "                        num_evil_edges_flag = False\n",
        "\n",
        "                    for n, i in tqdm(enumerate(ori_graph_ids)):\n",
        "                        f.copy(str(i), g, name=str(n))\n",
        "                        if self.add_nfeat_ones:\n",
        "                            g[str(n)].create_dataset('x',\n",
        "                                                     shape=(g[str(n)].attrs['num_nodes'], 1),\n",
        "                                                     dtype='f4',\n",
        "                                                     data=np.ones((g[str(n)].attrs['num_nodes'], 1)))\n",
        "\n",
        "                        num_nodes_sum += f[str(i)].attrs['num_nodes']\n",
        "                        num_edges_sum += f[str(i)].attrs['num_edges']\n",
        "                        num_evils_sum += f[str(i)].attrs['num_evils']\n",
        "                        if num_evil_edges_flag:\n",
        "                            num_evil_edges_sum += f[str(i)].attrs['num_evil_edges']\n",
        "\n",
        "                    g.attrs['num_graphs'] = n + 1\n",
        "                    g.attrs['num_nodes_avg'] = num_nodes_sum / (n + 1)\n",
        "                    g.attrs['num_edges_avg'] = num_edges_sum / (n + 1)\n",
        "                    g.attrs['num_evils_avg'] = num_evils_sum / (n + 1)\n",
        "                    if num_evil_edges_flag:\n",
        "                        g.attrs['num_evil_edges_avg'] = num_evil_edges_sum / (n + 1)\n",
        "                    g.attrs['is_directed'] = f.attrs['is_directed']\n",
        "                    g.attrs['contains_self_loops'] = f.attrs['contains_self_loops']\n",
        "                    g.attrs['ori_graph_ids'] = ori_graph_ids\n",
        "\n",
        "                print('{} split --- number of graphs: {}, data saved at {}.'.format(split, n + 1, path))\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_graphs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.data_type == 'dict':\n",
        "            graph_dict = self.data[str(index)]\n",
        "        elif self.data_type == 'file':\n",
        "            if self.data is None:\n",
        "                # only open once in each process\n",
        "                self.data = h5py.File(self.path, 'r')\n",
        "            graph_dict = h5group_to_dict(self.data[str(index)])\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        # graph_format == 'pyg':\n",
        "        return build_graph_from_dict_pyg(graph_dict)\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self.num_graphs):\n",
        "            yield self[i]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(topology: {self.name} | split: {self.split} | ' \\\n",
        "               f'#graphs: {len(self)} | graph format: {self.graph_format})'\n"
      ],
      "metadata": {
        "id": "D3ulTrWnD9Rd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Eval"
      ],
      "metadata": {
        "id": "igYJ4y99D-bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Metrics"
      ],
      "metadata": {
        "id": "z_dcpDhREijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(target, pred, label):\n",
        "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    tp = np.sum((target==label) & (pred==label))\n",
        "    fp = np.sum((target!=label) & (pred==label))\n",
        "    fn = np.sum((pred!=label) & (target==label))\n",
        "    \n",
        "    if tp+fp==0 or tp+fn==0:\n",
        "      return np.nan\n",
        "\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    \n",
        "    if precision+recall==0:\n",
        "      return np.nan\n",
        "      \n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def f1_macro(pred, target):\n",
        "    return np.mean([f1(target, pred, label) for label in range(0, 2)])\n",
        "\n",
        "\n",
        "def accuracy(pred, target):\n",
        "    return (pred == target).sum().item() / len(target)\n",
        "\n",
        "\n",
        "def true_positive(pred, target):\n",
        "    return (target[pred == 1] == 1).sum().item()\n",
        "\n",
        "\n",
        "def false_positive(pred, target):\n",
        "    return (target[pred == 1] == 0).sum().item()\n",
        "\n",
        "\n",
        "def true_negative(pred, target):\n",
        "    return (target[pred == 0] == 0).sum().item()\n",
        "\n",
        "\n",
        "def false_negative(pred, target):\n",
        "    return (target[pred == 0] == 1).sum().item()\n",
        "\n",
        "\n",
        "def recall(pred, target):\n",
        "    try:\n",
        "        return true_positive(pred, target) / (target == 1).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def precision(pred, target):\n",
        "    try:\n",
        "        prec = true_positive(pred, target) / (pred == 1).sum().item()\n",
        "        return prec\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def f1_score(pred, target):\n",
        "    prec = precision(pred, target)\n",
        "    rec = recall(pred, target)\n",
        "    try:\n",
        "        return 2 * (prec * rec) / (prec + rec)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def false_positive_rate(pred, target):\n",
        "    try:\n",
        "        return false_positive(pred, target) / (target == 0).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n",
        "\n",
        "\n",
        "def false_negative_rate(pred, target):\n",
        "    try:\n",
        "        return false_negative(pred, target) / (target == 1).sum().item()\n",
        "    except:  # divide by zero\n",
        "        return -1\n"
      ],
      "metadata": {
        "id": "BN-QIjMREw-7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Evaluation"
      ],
      "metadata": {
        "id": "fTEnWpOOEZW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metrics(target, pred_prob, threshold=0.5):\n",
        "    if isinstance(target, torch.Tensor):\n",
        "        target = target.cpu().numpy()\n",
        "    if isinstance(pred_prob, torch.Tensor):\n",
        "        pred_prob = pred_prob.cpu().numpy()\n",
        "\n",
        "    pred = (pred_prob >= threshold).astype(int)\n",
        "\n",
        "    acc = accuracy(pred, target)\n",
        "    fpr = false_positive_rate(pred, target)\n",
        "    fnr = false_negative_rate(pred, target)\n",
        "    rec = recall(pred, target)\n",
        "    prc = precision(pred, target)\n",
        "    f1 = f1_score(pred, target)\n",
        "    f1macro = f1_macro(pred, target)\n",
        "    result_dict = {'acc': acc, 'fpr': fpr, 'fnr': fnr, 'rec': rec, 'prc': prc, 'f1': f1, 'f1_macro': f1macro}\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "\n",
        "def dict_value_add(dict1, dict2):\n",
        "    result = {key: dict1.get(key, 0) + dict2.get(key, 0)\n",
        "              for key in set(dict1) | set(dict2)}\n",
        "    return result\n",
        "\n",
        "\n",
        "def dict_value_div(dict, n):\n",
        "    result = {key: value / n for key, value in dict.items()}\n",
        "    return result\n",
        "\n",
        "\n",
        "def eval_predictor(dataset, predictor):\n",
        "    result_dict_avg = {}\n",
        "    loss_avg = 0\n",
        "\n",
        "    for data in dataset:\n",
        "        # prediction\n",
        "        try:\n",
        "            pred_prob, loss = predictor(data)\n",
        "            loss_avg += loss\n",
        "        except ValueError:  # if \"too many values to unpack\"\n",
        "            pred_prob = predictor(data)\n",
        "\n",
        "        # get the ground truth target\n",
        "        # graph_format == 'pyg':\n",
        "        target = data.y\n",
        "\n",
        "        # compute the evaluation metrics\n",
        "        result_dict = eval_metrics(target, pred_prob)\n",
        "\n",
        "        result_dict_avg = dict_value_add(result_dict_avg, result_dict)\n",
        "\n",
        "    # average the metrics across all graphs in the dataset as final results\n",
        "    result_dict_avg = dict_value_div(result_dict_avg, len(dataset))\n",
        "    loss_avg = loss_avg / len(dataset)\n",
        "\n",
        "    return result_dict_avg, loss_avg\n",
        "\n",
        "\n",
        "# =================================================================================================================\n",
        "# some examples of the 'predictor' model wrapper to be fed into the above evaluation function (for PyG Data format)\n",
        "# =================================================================================================================\n",
        "class PygRandomPredictor:\n",
        "    def __init__(self):\n",
        "        # torch.manual_seed(0)\n",
        "        pass\n",
        "\n",
        "    def __call__(self, data):\n",
        "        pred_prob = torch.rand(len(data.y))\n",
        "        return pred_prob\n",
        "\n",
        "\n",
        "class PygModelPredictor:\n",
        "    def __init__(self, model, loss_fcn=torch.nn.CrossEntropyLoss()):\n",
        "        self.model = model\n",
        "        self.loss_fcn = loss_fcn\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def __call__(self, data):\n",
        "        self.model.eval()\n",
        "        data = data.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            # custom the below line to adjust to your model's input format for forward pass\n",
        "            out = self.model(data.x, data.edge_index)\n",
        "            loss = self.loss_fcn(out, data.y.long())\n",
        "            pred_prob = torch.softmax(out, dim=1)[:, 1]\n",
        "        return pred_prob, loss.float()\n"
      ],
      "metadata": {
        "id": "pqjalkx1EmHc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Optimization"
      ],
      "metadata": {
        "id": "_axvEgpiERb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Early stop"
      ],
      "metadata": {
        "id": "k1Anl1YEE8N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode='min', verbose=False, logger=None):\n",
        "\n",
        "        assert mode in ['min', 'max']\n",
        "\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.verbose = verbose\n",
        "        self.logging = logger.info if logger else print\n",
        "        self.counter = 0\n",
        "        self.best = None\n",
        "        self.improved = False\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_metric):\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = val_metric\n",
        "            self.improved = True\n",
        "        elif (self.mode == 'min' and val_metric < self.best) or \\\n",
        "             (self.mode == 'max' and val_metric > self.best):\n",
        "            self.best = val_metric\n",
        "            self.improved = True\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.improved = False\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                self.logging(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n"
      ],
      "metadata": {
        "id": "ybOfGAdhFElv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Train Utils"
      ],
      "metadata": {
        "id": "7bEShFynFBF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def time_since(start):\n",
        "    now = time.time()\n",
        "    s = now - start\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    h = math.floor(m / 60)\n",
        "    m -= h * 60\n",
        "    if h == 0:\n",
        "        if m == 0:\n",
        "            return '%ds' % s\n",
        "        else:\n",
        "            return '%dm %ds' % (m, s)\n",
        "    else:\n",
        "        return '%dh %dm %ds' % (h, m, s)\n"
      ],
      "metadata": {
        "id": "wN2aiWdXFJa8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Model"
      ],
      "metadata": {
        "id": "5fvRugR_E1QL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. Activation "
      ],
      "metadata": {
        "id": "8rh6Ca1XE6ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(act, negative_slope=0.2):\n",
        "    activations = nn.ModuleDict([\n",
        "        ['lrelu', nn.LeakyReLU(negative_slope)],\n",
        "        ['relu', nn.ReLU()],\n",
        "        ['elu', nn.ELU()],\n",
        "        ['none', nn.Identity()],\n",
        "    ])\n",
        "    return activations[act]"
      ],
      "metadata": {
        "id": "bzEVz1VwFjKK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2. GCN base model"
      ],
      "metadata": {
        "id": "axjMlKrAFX-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NodeModelBase(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, deg_norm='none', aggr='add',\n",
        "                 *args, **kwargs):\n",
        "        assert aggr in ['add', 'mean', 'max']\n",
        "\n",
        "        super(NodeModelBase, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.deg_norm = deg_norm\n",
        "        self.aggr = aggr\n",
        "\n",
        "    @staticmethod\n",
        "    def degnorm_const(edge_index=None, num_nodes=None, deg=None, method='sm', device=None):\n",
        "\n",
        "        if device is None and edge_index is not None:\n",
        "            device = edge_index.device\n",
        " \n",
        "        edge_weight = torch.ones((edge_index.size(1),), device=device)\n",
        "\n",
        "        row, col = edge_index\n",
        "        \n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "\n",
        "        # 'sm'\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        norm = (deg_inv_sqrt[row] * deg_inv_sqrt[col])  # size (E,)\n",
        "\n",
        "        return norm\n",
        "\n",
        "    def forward(self, x, edge_index, deg=None):\n",
        "        return x\n",
        "\n",
        "    def num_parameters(self):\n",
        "        if not hasattr(self, 'num_para'):\n",
        "            self.num_para = sum([p.nelement() for p in self.parameters()])\n",
        "        return self.num_para\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{} (in_channels: {}, out_channels: {}, deg_norm: {},' \\\n",
        "               'aggr: {} | number of parameters: {})'.format(\n",
        "            self.__class__.__name__, self.in_channels, self.out_channels,\n",
        "            self.deg_norm, self.aggr, self.num_parameters())\n",
        "\n",
        "class NodeModelMLP(NodeModelBase):\n",
        "    def __init__(self, in_channels, out_channels, deg_norm='sm', aggr='add',\n",
        "                 bias=True, mlp_nlay=2, mlp_nhid=32, mlp_act='relu'):\n",
        "        super(NodeModelMLP, self).__init__(in_channels, out_channels, deg_norm, aggr)\n",
        "\n",
        "        in_features = in_channels\n",
        "\n",
        "        if mlp_nlay == 1:\n",
        "            self.mlp = nn.Linear(in_features, out_channels, bias=bias)\n",
        "        elif mlp_nlay >= 2:\n",
        "            self.mlp = [nn.Linear(in_features, mlp_nhid, bias=bias)]\n",
        "            for i in range(mlp_nlay - 1):\n",
        "                self.mlp.append(activation(mlp_act))\n",
        "                if i < mlp_nlay - 2:\n",
        "                    self.mlp.append(nn.Linear(mlp_nhid, mlp_nhid, bias=bias))\n",
        "                else:\n",
        "                    # last layer, and we do not apply non-linear activation after\n",
        "                    self.mlp.append(nn.Linear(mlp_nhid, out_channels, bias=bias))\n",
        "            self.mlp = nn.Sequential(*self.mlp)\n",
        "\n",
        "        # self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self, initrange=0.1):\n",
        "        # TODO: this only works for 1-layer mlp\n",
        "        nn.init.uniform_(self.mlp.weight, -initrange, initrange)\n",
        "        if self.mlp.bias is not None:\n",
        "            nn.init.constant_(self.mlp.bias, 0)\n",
        "\n",
        "        # self.mlp.reset_parameters()    # this was done automatically when nn.Linear class was initialized\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, deg=None, **kwargs):\n",
        "        # calculate the degree normalization factors, of size (E,)\n",
        "        # or of size (N,) when `self.deg_norm` == 'rw' and `edge_weight` == None\n",
        "        norm = self.degnorm_const(edge_index, num_nodes=x.size(0), deg=deg,\n",
        "                                  method=self.deg_norm, device=x.device)\n",
        "\n",
        "        # 'chuan hoa sm'\n",
        "        # lift the features to source nodes, resulting size (E, C_out)\n",
        "        x_j = torch.index_select(x, 0, edge_index[0])\n",
        "        x_j = x_j * norm.view(-1, 1)  # norm.view(-1, 1) second dim set to 1 for broadcasting\n",
        "\n",
        "        x_j = self.mlp(x_j)  # size (E, C_out)\n",
        "\n",
        "        # aggregate the features into nodes, resulting size (N, C_out)\n",
        "        # x_o = scatter_(self.aggr, x_j, edge_index[1], dim_size=x.size(0))\n",
        "        x = scatter_(self.aggr, x_j, edge_index[1], dim_size=x.size(0))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "cyFZpJbHFmYs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3. GCN model"
      ],
      "metadata": {
        "id": "HWrzKUrFFcNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, in_channels, enc_sizes, num_classes, non_linear='relu', non_linear_layer_wise='none',\n",
        "                 residual_hop=None, dropout=0.0, final_type='none'):\n",
        "        assert final_type in ['none', 'proj']\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.enc_sizes = [in_channels, *enc_sizes]\n",
        "        self.num_layers = len(self.enc_sizes) - 1\n",
        "        self.num_classes = num_classes\n",
        "        self.residual_hop = residual_hop\n",
        "        self.non_linear_layer_wise = non_linear_layer_wise\n",
        "        self.final_type = final_type\n",
        "        \n",
        "        import torch\n",
        "        import torch.nn.functional as F\n",
        "        from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
        "        from torch_geometric.nn import GCNConv, GINConv\n",
        "        from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "        dim_h = 32\n",
        "        self.conv0 = GINConv(\n",
        "            Sequential(Linear(1, dim_h),\n",
        "                       BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h),\n",
        "                       BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv4 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv5 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv6 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv7 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv8 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv9 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv10 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))        \n",
        "        self.conv11 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        \n",
        "        self.gcn_net = nn.ModuleList([self.conv0, self.conv1, self.conv2, self.conv3, \n",
        "                                      self.conv4, self.conv5, self.conv6, self.conv7, \n",
        "                                      self.conv8, self.conv9, self.conv10, self.conv11])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if residual_hop is not None and residual_hop > 0:\n",
        "            self.residuals = nn.ModuleList([nn.Linear(self.enc_sizes[i], self.enc_sizes[j], bias=False)\n",
        "                                            if self.enc_sizes[i] != self.enc_sizes[j]\n",
        "                                            else\n",
        "                                            nn.Identity()\n",
        "                                            for i, j in zip(range(0, len(self.enc_sizes), residual_hop),\n",
        "                                                            range(residual_hop, len(self.enc_sizes), residual_hop))])\n",
        "            self.num_residuals = len(self.residuals)\n",
        "\n",
        "        self.non_linear = activation(non_linear)\n",
        "\n",
        "        if self.final_type == 'none':\n",
        "            self.final = nn.Identity()\n",
        "        elif self.final_type == 'proj':\n",
        "            self.final = nn.Linear(self.enc_sizes[-1], num_classes)\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for net in self.gcn_net:\n",
        "            net.reset_parameters()\n",
        "        if self.residual_hop is not None:\n",
        "            for net in self.residuals:\n",
        "                net.reset_parameters()\n",
        "        if self.final_type != 'none':\n",
        "            self.final.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, deg=None):\n",
        "        xr = None\n",
        "        add_xr_at = -1\n",
        "\n",
        "        for n, net in enumerate(self.gcn_net):\n",
        "            # pass to a GCN layer with non-linear activation\n",
        "            xo = net(x, edge_index)\n",
        "            xo = self.dropout(xo)\n",
        "            # deal with residual connections\n",
        "            if self.residual_hop is not None and self.residual_hop > 0:\n",
        "                if n % self.residual_hop == 0 and (n // self.residual_hop) < self.num_residuals:\n",
        "                    xr = self.residuals[n // self.residual_hop](x)\n",
        "                    add_xr_at = n + self.residual_hop - 1\n",
        "                if n == add_xr_at:\n",
        "                    if n < self.num_layers - 1:  # before the last layer\n",
        "                        # non_linear is applied both after each layer (by default: 'none') and after residual sum\n",
        "                        xo = self.non_linear(xo + xr)\n",
        "                    else:  # the last layer (potentially the output layer)\n",
        "                        if self.final_type == 'none':\n",
        "                            # no non_linear is important for binary classification since this is to be passed to sigmoid\n",
        "                            # function to calculate loss, and ReLU will directly kill all the negative parts\n",
        "                            xo = xo + xr\n",
        "                        else:\n",
        "                            xo = self.non_linear(xo + xr)\n",
        "            else:\n",
        "                if n < self.num_layers - 1:  # before the last layer\n",
        "                    xo = self.non_linear(xo)\n",
        "                else:\n",
        "                    if self.final_type == 'none':\n",
        "                        pass\n",
        "                    else:\n",
        "                        xo = self.non_linear(xo)\n",
        "\n",
        "            x = xo\n",
        "        # size of x: (B * N, self.enc_sizes[-1]) -> (B * N, num_classes)\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, aggr='add',\n",
        "                 bias=True, non_linear='relu'):\n",
        "        super().__init__()\n",
        "        self.gcn = NodeModelMLP(in_channels,\n",
        "                                out_channels,\n",
        "                                aggr=aggr,\n",
        "                                bias=bias)\n",
        "\n",
        "        self.non_linear = activation(non_linear)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, deg=None):\n",
        "        xo = self.gcn(x, edge_index, edge_attr, deg)\n",
        "        xo = self.non_linear(xo)\n",
        "        return xo\n",
        "\n"
      ],
      "metadata": {
        "id": "vLv8xNFcFqcT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. Split train/val/test"
      ],
      "metadata": {
        "id": "RRCYp7yuLxMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './data/botnet'\n",
        "data_name = 'chord' # 'chord', 'debru', 'kadem', 'leet', 'c2', 'p2p'\n",
        "batch_size = 2\n",
        "in_memory = False\n",
        "shuffle = False\n",
        "\n",
        "# ========== load the dataset\n",
        "print('loading dataset...')\n",
        "\n",
        "train_ds = BotnetDataset(name=data_name, root=data_dir, split='train',\n",
        "                         in_memory=bool(in_memory), graph_format='pyg')\n",
        "val_ds = BotnetDataset(name=data_name, root=data_dir, split='val',\n",
        "                       in_memory=bool(in_memory), graph_format='pyg')\n",
        "test_ds = BotnetDataset(name=data_name, root=data_dir, split='test',\n",
        "                        in_memory=bool(in_memory), graph_format='pyg')\n",
        "train_loader = GraphDataLoader(train_ds, batch_size=batch_size, shuffle=bool(shuffle), num_workers=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msBAy6CKLwl8",
        "outputId": "d3028d5f-e663-42d4-da3c-3f68247ffae2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII. Train"
      ],
      "metadata": {
        "id": "PSY11P9WEX5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== some default parameters =============\n",
        "devid = 0\n",
        "seed = 0\n",
        "logmode = 'w'\n",
        "log_interval = 96\n",
        "\n",
        "in_channels = 1\n",
        "enc_sizes = [32] * 12\n",
        "act = 'relu' # 'none', 'lrelu', 'relu', 'elu'\n",
        "layer_act = 'none' # 'none', 'lrelu', 'relu', 'elu'\n",
        "\n",
        "residual_hop = 1\n",
        "num_classes = 2\n",
        "final = 'proj'    # 'none', 'proj'\n",
        "\n",
        "deg_norm = 'sm'\n",
        "aggr = 'add' # 'add', 'mean', 'max'\n",
        "dropout = 0.0\n",
        "bias = True\n",
        "\n",
        "\n",
        "lr = 0.005 # learning rate\n",
        "weight_decay = 5e-4\n",
        "epochs = 20\n",
        "early_stop = True\n",
        "save_dir = './saved_models'\n",
        "save_name = 'temp.pt'\n",
        "\n",
        "# ====================================================\n",
        "\n",
        "def train(model, train_loader, val_dataset, test_dataset, optimizer, criterion,\n",
        "          scheduler=None):\n",
        "    device = next(model.parameters()).device\n",
        "    predictor = PygModelPredictor(model)\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=5, mode='min', verbose=True)\n",
        "\n",
        "    best_epoch = 0\n",
        "    min_avg_lost = np.inf\n",
        "    start = time.time()\n",
        "    for ep in range(epochs):\n",
        "        loss_avg_train = 0\n",
        "        num_train_graph = 0\n",
        "        model.train()\n",
        "        for n, batch in enumerate(train_loader):\n",
        "            batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = model(batch.x, batch.edge_index)\n",
        "            loss = criterion(x, batch.y.long())\n",
        "\n",
        "            loss_avg_train += float(loss)\n",
        "            num_train_graph += batch.num_graphs\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if num_train_graph % log_interval == 0 or n == len(train_loader) - 1:\n",
        "                with torch.no_grad():\n",
        "                    # pred = x.argmax(dim=1)\n",
        "                    pred_prob = torch.softmax(x, dim=1)[:, 1]\n",
        "                    y = batch.y.long()\n",
        "                    result_dict = eval_metrics(y, pred_prob)\n",
        "                print(f'epoch: {ep + 1}, passed number of graphs: {num_train_graph}, '\n",
        "                        f'train running loss: {loss_avg_train / num_train_graph:.5f} (passed time: {time_since(start)})')\n",
        "                print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict.items()]))\n",
        "\n",
        "        result_dict_avg, loss_avg = eval_predictor(val_dataset, predictor)\n",
        "        print(f'Validation --- epoch: {ep + 1}, loss: {loss_avg:.5f}')\n",
        "        print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(loss_avg)\n",
        "\n",
        "        if loss_avg < min_avg_lost:\n",
        "            torch.save(model, os.path.join(save_dir, save_name))\n",
        "            print(f'Better model saved at {os.path.join(save_dir, save_name)}.')\n",
        "            best_epoch = ep\n",
        "            min_avg_lost = loss_avg\n",
        "\n",
        "    best_model = torch.load(os.path.join(save_dir, save_name))\n",
        "    print('*' * 12 + f' best model obtained after epoch {best_epoch + 1}, '\n",
        "                       f'saved at {os.path.join(save_dir, save_name)} ' + '*' * 12)\n",
        "    \n",
        "    predictor = PygModelPredictor(best_model)\n",
        "\n",
        "    result_dict_avg, loss_avg = eval_predictor(test_dataset, predictor)\n",
        "    print(f'Testing --- loss: {loss_avg:.5f}')\n",
        "    print(' ' * 10 + ', '.join(['{}: {:.5f}'.format(k, v) for k, v in result_dict_avg.items()]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # ========== random seeds and device\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = torch.device(f'cuda:{devid}') if devid > -1 else torch.device('cpu')\n",
        "\n",
        "    # ========== logging setup\n",
        "    log_name = os.path.splitext(save_name)[0]\n",
        "    # logger = logging_config(__name__, folder=save_dir, name=log_name, filemode=logmode)\n",
        "    # logger = logging_config(os.path.basename(__file__), folder=save_dir, name=log_name, filemode=logmode)\n",
        "\n",
        "    print('python ' + ' '.join(sys.argv))\n",
        "    print('-' * 30)\n",
        "    #logger.info(args)\n",
        "    print('-' * 30)\n",
        "    print(time.ctime())\n",
        "    print('-' * 30)\n",
        "\n",
        "    # ========== define the model, optimizer, and loss\n",
        "\n",
        "    model = GCNModel(in_channels,\n",
        "                     enc_sizes,\n",
        "                     num_classes,\n",
        "                     non_linear=act,\n",
        "                     non_linear_layer_wise=layer_act,\n",
        "                     residual_hop=residual_hop,\n",
        "                     dropout=dropout,\n",
        "                     final_type=final,\n",
        "                     )\n",
        "\n",
        "    print('model ' + '-' * 10)\n",
        "    print(repr(model))\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=1)\n",
        "\n",
        "    # ========== train the model\n",
        "    train(model, train_loader, val_ds, test_ds, optimizer, criterion,\n",
        "          scheduler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iFDupzCdBl0",
        "outputId": "2b24b30d-d4f1-4aea-ee10-87511de2eb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-34393bdf-8a6f-4acf-8e6f-da0472dc9d5c.json\n",
            "------------------------------\n",
            "------------------------------\n",
            "Mon Oct 17 16:29:45 2022\n",
            "------------------------------\n",
            "model ----------\n",
            "GCNModel(\n",
            "  (conv0): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=1, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv1): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv2): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv3): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv4): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv5): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv6): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv7): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv8): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv9): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv10): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (conv11): GINConv(nn=Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "  ))\n",
            "  (gcn_net): ModuleList(\n",
            "    (0): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=1, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (1): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (2): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (3): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (4): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (5): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (6): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (7): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (8): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (9): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (10): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "    (11): GINConv(nn=Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (4): ReLU()\n",
            "    ))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            "  (residuals): ModuleList(\n",
            "    (0): Linear(in_features=1, out_features=32, bias=False)\n",
            "    (1): Identity()\n",
            "    (2): Identity()\n",
            "    (3): Identity()\n",
            "    (4): Identity()\n",
            "    (5): Identity()\n",
            "    (6): Identity()\n",
            "    (7): Identity()\n",
            "    (8): Identity()\n",
            "    (9): Identity()\n",
            "    (10): Identity()\n",
            "    (11): Identity()\n",
            "  )\n",
            "  (non_linear): ReLU()\n",
            "  (final): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n",
            "epoch: 1, passed number of graphs: 96, train running loss: 0.16798 (passed time: 16s)\n",
            "          acc: 0.93075, fpr: 0.00004, fnr: 1.00000, rec: 0.00000, prc: 0.00000, f1: 0.00000, f1_macro: nan\n",
            "epoch: 1, passed number of graphs: 192, train running loss: 0.14285 (passed time: 37s)\n",
            "          acc: 0.93020, fpr: 0.00000, fnr: 1.00000, rec: 0.00000, prc: -1.00000, f1: 0.00000, f1_macro: nan\n"
          ]
        }
      ]
    }
  ]
}